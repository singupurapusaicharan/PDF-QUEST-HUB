"""
Question-answering service for the PDF Quest API.
This file provides functions for answering questions about PDF documents using LangChain with free models.
"""
import os
import subprocess
import time
from sqlalchemy.orm import Session
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain.docstore.document import Document as LangchainDocument

from app.database import QAPair
from app.services.document_service import get_document_by_id, get_document_text
from app.config import OPENAI_API_KEY

# Flag to enable mock mode (set to False to use Ollama)
MOCK_MODE = False  # Ollama is now running, so we can use it

# Initialize Hugging Face embeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Check if Ollama is installed and running
def ensure_ollama_running():
    """
    Check if Ollama is installed and running.
    If not, provide instructions to the user.
    """
    try:
        # Try to connect to Ollama - using tinyllama model which is fast and small
        llm = Ollama(model="tinyllama")
        return True
    except Exception as e:
        print(f"Error connecting to Ollama: {str(e)}")
        print("\n" + "="*80)
        print("Ollama not detected. Please install and run Ollama:")
        print("1. Download and install Ollama from: https://ollama.com/download")
        print("2. Run 'ollama run tinyllama' in a terminal")
        print("3. Restart this application")
        print("="*80 + "\n")
        return False

# Try to ensure Ollama is running, otherwise use mock mode
if not MOCK_MODE:
    MOCK_MODE = not ensure_ollama_running()


def create_document_index(document_text):
    """
    Create a searchable index from document text using Hugging Face embeddings.
    
    Args:
        document_text (str): The text content of the document
        
    Returns:
        FAISS: A FAISS vector store containing the document chunks
    """
    if MOCK_MODE:
        # Return a simple mock object that supports similarity_search
        class MockVectorStore:
            def similarity_search(self, query, k=4):
                return [
                    LangchainDocument(page_content="This is a mock document chunk for testing purposes.")
                ]
        return MockVectorStore()
    
    # Split the text into chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    
    # Create documents from the text chunks
    chunks = text_splitter.split_text(document_text)
    documents = [LangchainDocument(page_content=chunk) for chunk in chunks]
    
    # Create a vector store from the documents using Hugging Face embeddings
    vector_store = FAISS.from_documents(documents, embeddings)
    
    return vector_store


def answer_question(document_id: int, question: str, db: Session):
    """
    Answer a question about a document using Ollama.
    
    Args:
        document_id (int): The ID of the document
        question (str): The question to answer
        db (Session): Database session
        
    Returns:
        dict: The answer and related information
        
    Raises:
        ValueError: If the document is not found
    """
    try:
        print(f"[DEBUG] Starting answer_question for document_id={document_id}, question='{question}'")
        
        # Get the document
        document = get_document_by_id(document_id, db)
        
        if not document:
            raise ValueError(f"Document with ID {document_id} not found")
        
        print(f"[DEBUG] Document found: {document.filename}")
        
        # Get the document text
        document_text = get_document_text(document_id, db)
        print(f"[DEBUG] Document text extracted, length: {len(document_text)} characters")
        
        if MOCK_MODE:
            # Generate a mock answer for testing
            answer = f"This is a mock answer to your question: '{question}'. In a real scenario, this would be generated by analyzing the document content using free language models."
        else:
            print("[DEBUG] Creating vector store...")
            # Create a vector store from the document text
            vector_store = create_document_index(document_text)
            
            print("[DEBUG] Searching for relevant documents...")
            # Search for relevant document chunks - reduced from 4 to 3 for faster processing
            relevant_docs = vector_store.similarity_search(question, k=3)
            
            # Create a prompt for the LLM - simplified for faster processing
            context = "\n\n".join([doc.page_content for doc in relevant_docs])
            print(f"[DEBUG] Context prepared, length: {len(context)} characters")
            
            prompt = f"""Based on the following context, answer the question directly and concisely. If you don't know the answer, say so.

Context:
{context}

Question: {question}

Answer (be direct and concise):"""
            
            print("[DEBUG] Initializing Ollama LLM...")
            # Create a language model with Ollama - using tinyllama which is fast and small
            llm = Ollama(model="tinyllama", temperature=0.7, timeout=60)
            
            # Generate the answer
            try:
                print("[DEBUG] Generating answer with LLM...")
                answer = llm.invoke(prompt)
                print(f"[DEBUG] Answer generated successfully, length: {len(answer)} characters")
            except Exception as llm_error:
                print(f"[ERROR] LLM Error: {str(llm_error)}")
                # Fallback to a simpler prompt if the main one fails
                simple_prompt = f"Based on this context: {context[:1000]}\n\nAnswer this question briefly: {question}"
                answer = llm.invoke(simple_prompt)
        
        print("[DEBUG] Storing QA pair in database...")
        # Store the question-answer pair in the database
        qa_pair = QAPair(
            document_id=document_id,
            question=question,
            answer=answer
        )
        
        db.add(qa_pair)
        db.commit()
        db.refresh(qa_pair)
        
        print("[DEBUG] QA pair stored successfully")
        
        # Return the answer and related information
        return {
            "question": question,
            "answer": answer,
            "document_id": document_id,
            "document_name": document.filename,
            "qa_pair_id": qa_pair.id
        }
    except Exception as e:
        # Log the error for debugging
        print(f"[ERROR] Error in answer_question: {str(e)}")
        import traceback
        traceback.print_exc()
        raise


def get_qa_history(document_id: int, db: Session, limit: int = 10):
    """
    Get the question-answer history for a document.
    
    Args:
        document_id (int): The ID of the document
        db (Session): Database session
        limit (int): Maximum number of records to return
        
    Returns:
        list: List of question-answer pairs
        
    Raises:
        ValueError: If the document is not found
    """
    # Get the document
    document = get_document_by_id(document_id, db)
    
    if not document:
        raise ValueError(f"Document with ID {document_id} not found")
    
    # Get the question-answer pairs
    qa_pairs = db.query(QAPair).filter(QAPair.document_id == document_id).order_by(QAPair.timestamp.desc()).limit(limit).all()
    
    # Convert to dictionaries
    return [qa_pair.to_dict() for qa_pair in qa_pairs]


def summarize_document(document_id: int, db: Session):
    """
    Generate a summary of a document using Ollama.
    
    Args:
        document_id (int): The ID of the document
        db (Session): Database session
        
    Returns:
        str: The summary of the document
        
    Raises:
        ValueError: If the document is not found
    """
    try:
        # Get the document text
        document_text = get_document_text(document_id, db)
        
        if MOCK_MODE:
            # Generate a mock summary for testing
            return "This is a mock summary of the document. In a real scenario, this would be generated by analyzing the document content using free language models."
        else:
            # Create a summary prompt
            prompt = f"""
            Please provide a concise summary of the following document:
            
            {document_text[:5000]}...
            
            Summary:
            """
            
            # Create a language model with Ollama - using phi model
            llm = Ollama(model="phi", timeout=60)
            
            # Generate the summary
            summary = llm.invoke(prompt)
            
            return summary
    except Exception as e:
        # Log the error for debugging
        print(f"Error in summarize_document: {str(e)}")
        raise
